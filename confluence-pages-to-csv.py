import requests, csv, re
from bs4 import BeautifulSoup as bs

base_url = '' # Enter base url e.g. 'https://yourname.atlassian.net/wiki'
root_page_id = '' # Enter root page ID (e.g. '3833641')
confluence_token = '' # Enter Confluence API token (see https://support.atlassian.com/atlassian-account/docs/manage-api-tokens-for-your-atlassian-account/)
# base_url = input('Enter base url (e.g. https://yourname.atlassian.net/wiki):\n')
# root_page_id = input('Enter root page ID (e.g. 3833641):\n')
# confluence_token = input('Enter Confluence API token:\n')
output_file = "output.csv"
limit = '100'
rows = [['id', 'title', 'date', 'body', 'attachments', 'tags']]
headers = {'Authorization': 'token ' + confluence_token}
root_api_link = f'{base_url}/rest/api/content/{root_page_id}/child/page?expand=body.export_view,history,children.attachment,children.page.body.export_view,children.page.children.attachment,children.page.history&limit={limit}'

# Function writes rows to csv file
def csv_writer(rows, path):
    with open(path, "w", newline='') as csv_file:
        writer = csv.writer(csv_file, delimiter=',')
        for line in rows:
            writer.writerow(line)

#generate next links
def links_generator(root_api_link):
    while True:
        r = requests.get(root_api_link, headers=headers)
        api_response = r.json()

        if 'next' in api_response['_links']:
            yield root_api_link
            root_api_link = base_url + api_response['_links']['next']
        else:
            yield root_api_link
            return

# process links and generate rows
def process_links(root_api_link, rows):
    
    def generate_rows(page, rows):
        
        # generate attachments list
        attachments_list = []
        
        for att in page['children']['attachment']['results']:
            attachments_list.append(base_url + att['_links']['download'])

        # Optional: remove some image by alt attribute
        soup = bs(page['body']['export_view']['value'], 'html.parser')
        logo_img = soup.find('img', {'alt' : 'image_alt_attr'})
        logo_img.decompose() if logo_img else logo_img
        
        # Optional: find table with meta information, generated by Confluence Tabular Metadata Plugin, extract keywords to list, delete this table
        metas = soup.find(class_='plugin-tabmeta-details')
        if metas != None:
            tags = metas.find('td', string='Keywords').next_sibling.string if metas.find('td', string='Keywords') else str()
            metas.decompose()
        else:
            tags = str()
        
        # Optional: remove square brackets ([ ]) from the title
        cleaned_title = re.sub(r'^\[.*\]\s', '', page['title'])
        cleaned_title = re.sub(r'^\[.*\]', '', cleaned_title)

        # Optional: remove "id" and "style" attributes
        cleaned_body = re.sub(r'id="[^"]+"', '', str(soup))
        cleaned_body = re.sub(r'style="[^"]+"', '', cleaned_body)
        
        # append rows to list
        rows.append([ 
            page['id'], 
            cleaned_title, 
            page['history']['createdDate'], 
            cleaned_body, 
            ';'.join(attachments_list),
            str(tags) ])
    
    
    while True:
        r = requests.get(root_api_link, headers=headers)
        api_response = r.json()['results']
        
        for page in api_response:
            generate_rows(page, rows)

            print(f'{len(rows) - 1} rows generated', end='\r', flush=True)

            if not page['children']['page']['results']:
                pass
            else:
                for subpage in page['children']['page']['results']:
                    generate_rows(subpage, rows)
        
        if 'next' in r.json()['_links']:
            root_api_link = base_url + r.json()['_links']['next']
        else:
            return rows

# write rows to csv
if __name__ == "__main__":
    csv_writer(process_links(root_api_link, rows), output_file)

print('\nDone!')